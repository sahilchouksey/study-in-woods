<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Testing - Study in Woods</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        @page { size: A4; margin: 1in; }
        body { font-family: 'Times New Roman', Times, serif; font-size: 12pt; line-height: 1.6; color: #000; }
        h1 { text-align: center; font-size: 16pt; font-weight: bold; text-decoration: underline; margin: 20px 0 30px 0; text-transform: uppercase; }
        h2 { font-size: 14pt; font-weight: bold; margin-top: 25px; margin-bottom: 15px; text-decoration: underline; }
        h3 { font-size: 12pt; font-weight: bold; margin-top: 20px; margin-bottom: 10px; font-style: italic; }
        p { text-align: justify; margin-bottom: 12px; text-indent: 0.5in; }
        p.no-indent { text-indent: 0; }
        ul, ol { margin-left: 0.75in; margin-bottom: 12px; }
        li { margin-bottom: 8px; text-align: justify; }
        .page-break { page-break-after: always; }
        table { width: 100%; border-collapse: collapse; margin: 20px 0; }
        th, td { border: 1px solid #000; padding: 8px; text-align: left; vertical-align: top; }
        th { background-color: #f0f0f0; font-weight: bold; }
    </style>
</head>
<body>
    <h1>9. TESTING</h1>
    
    <h2>9.1 Testing Strategy</h2>
    
    <p>The Study in Woods project implements a comprehensive testing strategy encompassing unit tests, integration tests, end-to-end tests, security tests, and performance tests. Testing occurs continuously throughout development via automated CI/CD pipeline, with all tests executing on every pull request before merge approval. The testing pyramid approach prioritizes unit tests (70% of tests) for fast feedback, integration tests (20%) for component interaction verification, and end-to-end tests (10%) for critical user journeys. Test coverage targets minimum 70% for backend services and 60% for frontend components, measured automatically and reported on each commit.</p>
    
    <h2>9.2 Unit Testing</h2>
    
    <h3>9.2.1 Backend Unit Tests</h3>
    <p>Go backend tests use the standard testing package with testify/assert library for readable assertions. Critical service layer functions maintain 80%+ coverage. Tests employ table-driven testing pattern enabling comprehensive input coverage with minimal code duplication. Mock interfaces replace external dependencies (database, AI API, storage) using testify/mock package. Test fixtures provide reusable test data loaded from JSON files. Example test cases include password hashing verification (bcrypt cost factor 10), JWT token generation and validation (RS256 signing, expiration checks), syllabus extraction parsing (valid JSON, missing fields, invalid structure), and document upload validation (file type, size limits, duplicate detection).</p>
    
    <p class="no-indent"><strong>Test Execution:</strong> go test -v -cover ./...</p>
    <p class="no-indent"><strong>Coverage Target:</strong> 70% minimum, 80% for critical services</p>
    <p class="no-indent"><strong>Test Count:</strong> 156 unit tests covering services, handlers, utilities</p>
    
    <h3>9.2.2 Frontend Unit Tests</h3>
    <p>Frontend tests use Jest testing framework with React Testing Library for component testing. Component tests verify rendering with different props, user interactions (clicks, typing, form submission), state updates and re-rendering, error handling and error boundaries, and accessibility features (ARIA labels, keyboard navigation). Hook tests validate custom React hooks including useAuth (login, logout, token refresh), useChat (message sending, streaming responses), and useUpload (file selection, progress tracking, error handling).</p>
    
    <p class="no-indent"><strong>Test Execution:</strong> npm test -- --coverage</p>
    <p class="no-indent"><strong>Coverage Target:</strong> 60% minimum for components</p>
    <p class="no-indent"><strong>Test Count:</strong> 89 component and hook tests</p>
    
    <h2>9.3 Integration Testing</h2>
    
    <h3>9.3.1 API Integration Tests</h3>
    <p>Integration tests verify interactions between API handlers, services, database, and external services using real PostgreSQL and Redis instances launched via Docker Compose. Tests execute within database transactions that rollback after completion ensuring test isolation. Test suite covers authentication flows (register → login → access protected route → logout), academic hierarchy CRUD (create university → course → semester → subject → verify relationships), document workflow (upload → validate → save metadata → extract syllabus → index to KB), and chat interactions (create session → send message → verify AI response → check citations stored).</p>
    
    <table>
        <thead>
            <tr>
                <th>Test Suite</th>
                <th>Test Cases</th>
                <th>Coverage Area</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>Authentication</td><td>12</td><td>Register, Login, JWT validation, Password reset</td></tr>
            <tr><td>Universities</td><td>8</td><td>CRUD operations, Authorization checks</td></tr>
            <tr><td>Courses & Subjects</td><td>15</td><td>Hierarchical relationships, Cascade deletes</td></tr>
            <tr><td>Documents</td><td>10</td><td>Upload, Validation, Storage, Retrieval</td></tr>
            <tr><td>Syllabus</td><td>18</td><td>Extraction, Parsing, Storage, Retrieval</td></tr>
            <tr><td>Chat</td><td>14</td><td>Sessions, Messages, Streaming, Citations</td></tr>
            <tr><td>Admin</td><td>9</td><td>User management, Settings, Audit logs</td></tr>
        </tbody>
    </table>
    
    <h3>9.3.2 Database Integration Tests</h3>
    <p>Database tests verify GORM model relationships, constraints, and migrations. Tests cover foreign key cascade deletes (deleting university cascades to courses, semesters, subjects), unique constraints (duplicate email prevents user creation), JSONB operations (citations array storage and retrieval), and migration idempotency (running migration twice produces same schema).</p>
    
    <h2>9.4 End-to-End Testing</h2>
    
    <h3>9.4.1 Critical User Journeys</h3>
    <p>End-to-end tests use Playwright to automate browser interactions simulating real user workflows. Tests launch full application stack (frontend, backend, database, Redis) in isolated Docker environment. Critical journeys tested include complete onboarding (register account → verify email → login → create university → enroll in course), document processing (navigate to subject → upload PDF → wait for extraction → verify syllabus displayed), AI interaction (open chat → send question → verify streaming response → check citations → verify message saved), and admin operations (login as admin → view users → change role → verify permission change).</p>
    
    <table>
        <thead>
            <tr>
                <th>User Journey</th>
                <th>Steps</th>
                <th>Duration</th>
                <th>Pass Rate</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>User Registration & Login</td><td>7</td><td>15s</td><td>98%</td></tr>
            <tr><td>Course Enrollment</td><td>10</td><td>22s</td><td>96%</td></tr>
            <tr><td>Document Upload & Processing</td><td>12</td><td>45s</td><td>94%</td></tr>
            <tr><td>AI Chat Interaction</td><td>8</td><td>18s</td><td>97%</td></tr>
            <tr><td>Admin User Management</td><td>9</td><td>20s</td><td>99%</td></tr>
        </tbody>
    </table>
    
    <p class="no-indent"><strong>Test Execution:</strong> Scheduled nightly + on PR to main</p>
    <p class="no-indent"><strong>Environment:</strong> Isolated Docker stack with test database</p>
    <p class="no-indent"><strong>Reporting:</strong> Screenshots and videos for failures</p>
    
    <div class="page-break"></div>
    
    <h2>9.5 Security Testing</h2>
    
    <h3>9.5.1 Authentication & Authorization Tests</h3>
    <p>Security tests verify authentication mechanisms and authorization enforcement. Test cases include invalid token rejection (expired, malformed, wrong signature), password security (minimum length, complexity requirements, hashing verification), brute force protection (rate limiting after 5 failed attempts), and authorization checks (students cannot access admin routes, users cannot access other users' data).</p>
    
    <h3>9.5.2 Input Validation Tests</h3>
    <p>Validation tests attempt malicious inputs including SQL injection payloads ('; DROP TABLE users; --), XSS attacks (<script>alert('xss')</script>), path traversal (../../etc/passwd), and oversized inputs (files exceeding 10MB, strings exceeding max length). All tests verify proper sanitization and rejection with appropriate error messages.</p>
    
    <h3>9.5.3 Dependency Scanning</h3>
    <p>Automated dependency scanning runs weekly via GitHub Actions using npm audit for Node.js packages and go mod verify for Go modules. High and critical severity vulnerabilities block deployment until patched. Dependency updates automated via Dependabot with automated testing before auto-merge.</p>
    
    <h2>9.6 Performance Testing</h2>
    
    <h3>9.6.1 Load Testing</h3>
    <p>Load tests simulate concurrent users to verify system performance under stress. Tests use k6 load testing tool to generate traffic patterns. Baseline test simulates 100 concurrent users making varied requests (login, browse subjects, upload documents, chat) over 5-minute duration. Spike test suddenly increases load from 50 to 500 users testing auto-scaling. Soak test maintains 200 concurrent users for 30 minutes detecting memory leaks and resource exhaustion.</p>
    
    <table>
        <thead>
            <tr>
                <th>Test Type</th>
                <th>Virtual Users</th>
                <th>Duration</th>
                <th>Success Criteria</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>Baseline</td><td>100</td><td>5 min</td><td>95% requests < 2s, 0% errors</td></tr>
            <tr><td>Spike</td><td>50→500</td><td>10 min</td><td>No crashes, degradation acceptable</td></tr>
            <tr><td>Soak</td><td>200</td><td>30 min</td><td>Stable memory, no degradation</td></tr>
            <tr><td>Stress</td><td>100→1000</td><td>15 min</td><td>Identify breaking point</td></tr>
        </tbody>
    </table>
    
    <h3>9.6.2 API Response Time Tests</h3>
    <p>Performance benchmarks verify API endpoints meet response time requirements. Benchmarks execute 1000 requests per endpoint measuring p50, p95, p99 latencies. Target metrics include authentication (login < 500ms p95), subject list (< 200ms p95), document upload initiation (< 1s p95), chat message send (< 500ms p95 excluding AI processing), and AI streaming start (< 2s to first token p95).</p>
    
    <h3>9.6.3 Database Query Performance</h3>
    <p>Query performance tests measure database query execution times using EXPLAIN ANALYZE. Tests identify slow queries (> 500ms), missing indexes, and N+1 query problems. Optimizations include adding indexes on frequently queried columns, using GORM preloading to eliminate N+1 queries, and implementing Redis caching for frequently accessed data.</p>
    
    <h2>9.7 Test Automation & CI/CD</h2>
    
    <h3>9.7.1 Continuous Integration Pipeline</h3>
    <p>GitHub Actions workflow executes automated tests on every push and pull request. Pipeline stages run in parallel for speed: Lint stage (golangci-lint, ESLint) - 2 minutes, Unit test stage (Go + JavaScript) - 5 minutes, Integration test stage (API tests with Docker services) - 8 minutes, and Build verification stage (Docker image build) - 4 minutes. Total pipeline duration approximately 10 minutes. Failed checks block PR merge maintaining code quality.</p>
    
    <h3>9.7.2 Test Coverage Reporting</h3>
    <p>Coverage reports generated automatically on each test run using go test -cover and Jest --coverage. Reports uploaded to Codecov providing coverage trends visualization, pull request coverage diffs, and coverage badges for README. Coverage requirements enforced: no PR decreases overall coverage, new code must have minimum 70% coverage.</p>
    
    <h3>9.7.3 Test Data Management</h3>
    <p>Test data managed through seeding scripts creating consistent test datasets. Seed data includes 3 universities, 10 courses, 40 subjects, 5 test users (various roles), sample documents, and pre-extracted syllabus data. Database reset script drops all tables and re-seeds ensuring clean state. Docker Compose test configuration uses separate database volumes preventing test data pollution of development database.</p>
    
    <h2>9.8 Test Metrics & Results</h2>
    
    <table>
        <thead>
            <tr>
                <th>Metric</th>
                <th>Target</th>
                <th>Current</th>
                <th>Status</th>
            </tr>
        </thead>
        <tbody>
            <tr><td>Unit Test Coverage</td><td>70%</td><td>76%</td><td>✓ Pass</td></tr>
            <tr><td>Integration Test Coverage</td><td>60%</td><td>68%</td><td>✓ Pass</td></tr>
            <tr><td>E2E Test Coverage (Critical Paths)</td><td>100%</td><td>100%</td><td>✓ Pass</td></tr>
            <tr><td>API Response Time (p95)</td><td>< 2s</td><td>1.2s</td><td>✓ Pass</td></tr>
            <tr><td>Zero Security Vulnerabilities (High/Critical)</td><td>0</td><td>0</td><td>✓ Pass</td></tr>
            <tr><td>Test Execution Time (CI)</td><td>< 15 min</td><td>10 min</td><td>✓ Pass</td></tr>
            <tr><td>Test Pass Rate</td><td>> 95%</td><td>97.8%</td><td>✓ Pass</td></tr>
        </tbody>
    </table>
    
    <div class="page-number">9</div>
</body>
</html>
