package services

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"strings"
	"sync"
	"time"

	"github.com/sahilchouksey/go-init-setup/model"
	"github.com/sahilchouksey/go-init-setup/services/digitalocean"
	"github.com/sahilchouksey/go-init-setup/utils"
	"gorm.io/gorm"
)

// ChunkedSyllabusExtractor handles parallel chunked PDF syllabus extraction
type ChunkedSyllabusExtractor struct {
	db              *gorm.DB
	inferenceClient *digitalocean.InferenceClient
	spacesClient    *digitalocean.SpacesClient
	pdfExtractor    *PDFExtractor

	// Configuration
	maxConcurrent int           // Max parallel LLM calls (default: 5)
	maxRetries    int           // Max retries per chunk (default: 5)
	pagesPerChunk int           // Pages per chunk (default: 4)
	overlapPages  int           // Overlap between chunks (default: 1)
	chunkTimeout  time.Duration // Timeout per chunk (default: 2 min)
	mergeTimeout  time.Duration // Timeout for merge call (default: 3 min)
}

// ChunkedExtractorConfig holds configuration for the chunked extractor
type ChunkedExtractorConfig struct {
	MaxConcurrent int
	MaxRetries    int
	PagesPerChunk int
	OverlapPages  int
	ChunkTimeout  time.Duration
	MergeTimeout  time.Duration
}

// DefaultChunkedExtractorConfig returns default configuration
func DefaultChunkedExtractorConfig() ChunkedExtractorConfig {
	return ChunkedExtractorConfig{
		MaxConcurrent: 5,
		MaxRetries:    5,
		PagesPerChunk: 3, // Reduced from 4 to 3 for faster processing
		OverlapPages:  1,
		ChunkTimeout:  3 * time.Minute, // Increased from 2 to 3 minutes
		MergeTimeout:  3 * time.Minute,
	}
}

// ChunkResult holds the extraction result for a single chunk
type ChunkResult struct {
	ChunkIndex  int
	PageRange   PageRange
	Subjects    []SubjectExtractionResult
	RawResponse string
	Error       error
	Retries     int
}

// NewChunkedSyllabusExtractor creates a new chunked syllabus extractor
func NewChunkedSyllabusExtractor(
	db *gorm.DB,
	inferenceClient *digitalocean.InferenceClient,
	spacesClient *digitalocean.SpacesClient,
	pdfExtractor *PDFExtractor,
	config ChunkedExtractorConfig,
) *ChunkedSyllabusExtractor {
	// Apply defaults for zero values
	if config.MaxConcurrent <= 0 {
		config.MaxConcurrent = 5
	}
	if config.MaxRetries <= 0 {
		config.MaxRetries = 5
	}
	if config.PagesPerChunk <= 0 {
		config.PagesPerChunk = 4
	}
	if config.OverlapPages < 0 {
		config.OverlapPages = 1
	}
	if config.ChunkTimeout <= 0 {
		config.ChunkTimeout = 2 * time.Minute
	}
	if config.MergeTimeout <= 0 {
		config.MergeTimeout = 3 * time.Minute
	}

	return &ChunkedSyllabusExtractor{
		db:              db,
		inferenceClient: inferenceClient,
		spacesClient:    spacesClient,
		pdfExtractor:    pdfExtractor,
		maxConcurrent:   config.MaxConcurrent,
		maxRetries:      config.MaxRetries,
		pagesPerChunk:   config.PagesPerChunk,
		overlapPages:    config.OverlapPages,
		chunkTimeout:    config.ChunkTimeout,
		mergeTimeout:    config.MergeTimeout,
	}
}

// ExtractSyllabusChunked performs parallel chunked extraction from a document
func (c *ChunkedSyllabusExtractor) ExtractSyllabusChunked(
	ctx context.Context,
	document *model.Document,
	pdfContent []byte,
) ([]*model.Syllabus, error) {
	// 1. Get page count
	pageCount, err := c.pdfExtractor.GetPageCount(pdfContent)
	if err != nil {
		return nil, fmt.Errorf("failed to get page count: %w", err)
	}

	log.Printf("ChunkedExtractor: PDF has %d pages, starting chunked extraction", pageCount)

	// 2. Calculate chunks
	chunkConfig := ChunkConfig{
		PagesPerChunk: c.pagesPerChunk,
		OverlapPages:  c.overlapPages,
	}
	chunks := c.pdfExtractor.CalculateChunks(pageCount, chunkConfig)

	if len(chunks) == 0 {
		return nil, fmt.Errorf("no chunks calculated for PDF with %d pages", pageCount)
	}

	log.Printf("ChunkedExtractor: Processing %d chunks in parallel (max %d concurrent)", len(chunks), c.maxConcurrent)

	// 3. Process chunks in parallel
	chunkResults := c.processChunksParallel(ctx, pdfContent, chunks, pageCount)

	// 4. Check failure rate
	failedChunks := 0
	for _, result := range chunkResults {
		if result.Error != nil {
			failedChunks++
			log.Printf("ChunkedExtractor: Chunk %d (pages %d-%d) failed: %v",
				result.ChunkIndex, result.PageRange.Start, result.PageRange.End, result.Error)
		}
	}

	failureRate := float64(failedChunks) / float64(len(chunks)) * 100
	log.Printf("ChunkedExtractor: %d/%d chunks failed (%.1f%%)", failedChunks, len(chunks), failureRate)

	if failedChunks == len(chunks) {
		return nil, fmt.Errorf("all chunks failed to extract. Please re-upload the file")
	}

	if failureRate > 50 {
		return nil, fmt.Errorf("extraction failed for too many chunks (%.1f%%). Please re-upload the file", failureRate)
	}

	// 5. Merge and deduplicate results
	mergedResult, err := c.mergeAndDeduplicate(ctx, chunkResults)
	if err != nil {
		return nil, fmt.Errorf("failed to merge chunk results: %w", err)
	}

	if len(mergedResult.Subjects) == 0 {
		return nil, fmt.Errorf("no subjects extracted from document")
	}

	log.Printf("ChunkedExtractor: Merged %d subjects from chunks", len(mergedResult.Subjects))

	// 6. Save to database
	syllabuses, err := c.saveMultiSubjectSyllabusData(ctx, document, mergedResult)
	if err != nil {
		return nil, fmt.Errorf("failed to save syllabus data: %w", err)
	}

	return syllabuses, nil
}

// extractTopicsFromRawText intelligently extracts individual topics from raw_text
// Splits on common separators: comma, dash (–), semicolon, newline
func extractTopicsFromRawText(rawText string) []string {
	if rawText == "" {
		return []string{}
	}

	// Replace common separators with a delimiter
	text := rawText
	text = strings.ReplaceAll(text, " – ", "|") // em-dash
	text = strings.ReplaceAll(text, " - ", "|") // regular dash
	text = strings.ReplaceAll(text, "; ", "|")   // semicolon
	text = strings.ReplaceAll(text, ", ", "|")   // comma
	text = strings.ReplaceAll(text, "\n", "|")   // newline

	// Split by delimiter
	parts := strings.Split(text, "|")

	// Clean and filter topics
	var topics []string
	seenTopics := make(map[string]bool)
	
	for _, part := range parts {
		// Trim whitespace
		topic := strings.TrimSpace(part)
		
		// Skip empty, very short, or already seen topics
		if topic == "" || len(topic) < 3 {
			continue
		}
		
		// Normalize for deduplication (lowercase)
		normalized := strings.ToLower(topic)
		if seenTopics[normalized] {
			continue
		}
		
		seenTopics[normalized] = true
		topics = append(topics, topic)
	}

	// If no topics found, return the raw text as a single topic
	if len(topics) == 0 {
		return []string{rawText}
	}

	return topics
}

		}(i, chunk)
	}

	wg.Wait()
	return results
}

// extractChunkWithRetry extracts a chunk with retry logic
func (c *ChunkedSyllabusExtractor) extractChunkWithRetry(
	ctx context.Context,
	pdfContent []byte,
	pageRange PageRange,
	totalPages int,
	chunkIndex int,
) ChunkResult {
	var result ChunkResult
	result.ChunkIndex = chunkIndex
	result.PageRange = pageRange

	for attempt := 1; attempt <= c.maxRetries; attempt++ {
		result.Retries = attempt

		// Create timeout context for this chunk
		chunkCtx, cancel := context.WithTimeout(ctx, c.chunkTimeout)

		// Try extraction
		subjects, rawResponse, err := c.extractChunk(chunkCtx, pdfContent, pageRange, totalPages)
		cancel()

		if err == nil {
			result.Subjects = subjects
			result.RawResponse = rawResponse
			result.Error = nil
			return result
		}

		result.Error = err

		// Check if parent context is done
		if ctx.Err() != nil {
			result.Error = ctx.Err()
			return result
		}

		// Exponential backoff: 1s, 2s, 4s, 8s, 16s
		if attempt < c.maxRetries {
			backoff := time.Duration(1<<(attempt-1)) * time.Second
			log.Printf("ChunkedExtractor: Chunk %d attempt %d failed, retrying in %v: %v",
				chunkIndex+1, attempt, backoff, err)

			select {
			case <-ctx.Done():
				result.Error = ctx.Err()
				return result
			case <-time.After(backoff):
				// Continue to next retry
			}
		}
	}

	return result
}

// extractChunk extracts syllabus data from a single chunk
func (c *ChunkedSyllabusExtractor) extractChunk(
	ctx context.Context,
	pdfContent []byte,
	pageRange PageRange,
	totalPages int,
) ([]SubjectExtractionResult, string, error) {
	// Extract text for this page range
	text, err := c.pdfExtractor.ExtractPageRange(pdfContent, pageRange.Start, pageRange.End)
	if err != nil {
		return nil, "", fmt.Errorf("failed to extract text from pages %d-%d: %w", pageRange.Start, pageRange.End, err)
	}

	if len(text) < 50 {
		return nil, "", fmt.Errorf("insufficient text extracted from pages %d-%d", pageRange.Start, pageRange.End)
	}

	// Build SIMPLIFIED prompt for chunk extraction (minimal to reduce processing time)
	// Note: We extract topics in post-processing to avoid JSON truncation issues
	systemPrompt := `Extract syllabus info as JSON. Output ONLY: {"subjects":[{"subject_name":"","subject_code":"","total_credits":0,"units":[{"unit_number":1,"title":"","raw_text":"","topics":[{"topic_number":1,"title":""}]}],"books":[{"title":"","authors":"","book_type":"textbook"}]}]}`

	userPrompt := fmt.Sprintf("Pages %d-%d of %d. Extract subjects/units from:\n%s", pageRange.Start, pageRange.End, totalPages, text)

	// Call LLM with 4096 max_tokens for chunk processing
	response, err := c.inferenceClient.SimpleCompletion(
		ctx,
		systemPrompt,
		userPrompt,
		digitalocean.WithInferenceMaxTokens(4096),
		digitalocean.WithInferenceTemperature(0.0),
	)
	if err != nil {
		return nil, "", fmt.Errorf("LLM extraction failed: %w", err)
	}

	// Parse response
	var result SyllabusExtractionResult
	if err := utils.ExtractJSONTo(response, &result); err != nil {
		log.Printf("ChunkedExtractor: Failed to parse chunk response (length=%d): %v", len(response), err)
		return nil, response, fmt.Errorf("failed to parse LLM response: %w", err)
	}

	return result.Subjects, response, nil
}

// mergeAndDeduplicate merges chunk results and removes duplicates
// Groups subjects by name/code and merges each group separately in parallel
func (c *ChunkedSyllabusExtractor) mergeAndDeduplicate(
	ctx context.Context,
	chunkResults []ChunkResult,
) (*SyllabusExtractionResult, error) {
	// Collect all subjects from successful chunks
	var allPartialSubjects []SubjectExtractionResult
	for _, result := range chunkResults {
		if result.Error == nil && len(result.Subjects) > 0 {
			allPartialSubjects = append(allPartialSubjects, result.Subjects...)
		}
	}

	if len(allPartialSubjects) == 0 {
		return nil, fmt.Errorf("no subjects extracted from any chunk")
	}

	log.Printf("ChunkedExtractor: Merging %d partial subjects from chunks", len(allPartialSubjects))

	// Group subjects by subject_code (or name if code is empty)
	subjectGroups := c.groupSubjectsByCode(allPartialSubjects)
	log.Printf("ChunkedExtractor: Grouped into %d unique subjects", len(subjectGroups))

	// Merge each subject group in parallel (fast programmatic merge, no LLM calls)
	mergedSubjects := c.mergeSubjectGroupsParallel(subjectGroups)

	log.Printf("ChunkedExtractor: Parallel merge successful, %d unique subjects", len(mergedSubjects))
	return &SyllabusExtractionResult{Subjects: mergedSubjects}, nil
}

// groupSubjectsByCode groups subjects by their subject_code (or name if code is empty)
func (c *ChunkedSyllabusExtractor) groupSubjectsByCode(subjects []SubjectExtractionResult) map[string][]SubjectExtractionResult {
	groups := make(map[string][]SubjectExtractionResult)

	for _, subject := range subjects {
		key := subject.SubjectCode
		if key == "" {
			key = subject.SubjectName
		}

		groups[key] = append(groups[key], subject)
	}

	return groups
}

// mergeSubjectGroupsParallel merges subject groups in parallel
func (c *ChunkedSyllabusExtractor) mergeSubjectGroupsParallel(subjectGroups map[string][]SubjectExtractionResult) []SubjectExtractionResult {
	results := make([]SubjectExtractionResult, len(subjectGroups))
	var wg sync.WaitGroup

	i := 0
	for _, group := range subjectGroups {
		wg.Add(1)
		go func(idx int, subjects []SubjectExtractionResult) {
			defer wg.Done()
			results[idx] = c.mergeSubjectGroup(subjects)
		}(i, group)
		i++
	}

	wg.Wait()
	return results
}

// mergeSubjectGroup merges multiple partial extractions of the same subject
func (c *ChunkedSyllabusExtractor) mergeSubjectGroup(subjects []SubjectExtractionResult) SubjectExtractionResult {
	if len(subjects) == 1 {
		return subjects[0]
	}

	// Start with first subject as base
	merged := subjects[0]

	// Merge all other subjects into it
	for i := 1; i < len(subjects); i++ {
		subject := subjects[i]

		// Merge units
		merged.Units = c.mergeUnits(merged.Units, subject.Units)

		// Merge books
		merged.Books = c.mergeBooks(merged.Books, subject.Books)

		// Take higher credits
		if subject.TotalCredits > merged.TotalCredits {
			merged.TotalCredits = subject.TotalCredits
		}

		// Prefer longer/more detailed names
		if len(subject.SubjectName) > len(merged.SubjectName) {
			merged.SubjectName = subject.SubjectName
		}
	}

	return merged
}

// programmaticMerge performs a simple merge by subject_code
func (c *ChunkedSyllabusExtractor) programmaticMerge(subjects []SubjectExtractionResult) []SubjectExtractionResult {
	subjectMap := make(map[string]*SubjectExtractionResult)

	for _, subject := range subjects {
		key := subject.SubjectCode
		if key == "" {
			key = subject.SubjectName
		}

		if existing, ok := subjectMap[key]; ok {
			// Merge units
			existing.Units = c.mergeUnits(existing.Units, subject.Units)
			// Merge books
			existing.Books = c.mergeBooks(existing.Books, subject.Books)
			// Take higher credits
			if subject.TotalCredits > existing.TotalCredits {
				existing.TotalCredits = subject.TotalCredits
			}
		} else {
			// Copy to avoid modifying original
			copy := subject
			subjectMap[key] = &copy
		}
	}

	// Convert map to slice
	result := make([]SubjectExtractionResult, 0, len(subjectMap))
	for _, subject := range subjectMap {
		result = append(result, *subject)
	}

	return result
}

// mergeUnits merges unit lists, keeping the most complete version of each unit
func (c *ChunkedSyllabusExtractor) mergeUnits(existing, new []SyllabusUnitExtraction) []SyllabusUnitExtraction {
	unitMap := make(map[int]SyllabusUnitExtraction)

	// Add existing units
	for _, unit := range existing {
		unitMap[unit.UnitNumber] = unit
	}

	// Merge new units
	for _, unit := range new {
		if existingUnit, ok := unitMap[unit.UnitNumber]; ok {
			// Keep the one with more content
			if len(unit.RawText) > len(existingUnit.RawText) {
				unitMap[unit.UnitNumber] = unit
			} else if len(unit.Topics) > len(existingUnit.Topics) {
				unitMap[unit.UnitNumber] = unit
			}
		} else {
			unitMap[unit.UnitNumber] = unit
		}
	}

	// Convert to sorted slice
	result := make([]SyllabusUnitExtraction, 0, len(unitMap))
	for i := 1; i <= len(unitMap)+10; i++ { // +10 to handle gaps
		if unit, ok := unitMap[i]; ok {
			result = append(result, unit)
		}
	}

	return result
}

// mergeBooks merges book lists, removing duplicates by title
func (c *ChunkedSyllabusExtractor) mergeBooks(existing, new []BookReferenceExtraction) []BookReferenceExtraction {
	bookMap := make(map[string]BookReferenceExtraction)

	for _, book := range existing {
		bookMap[book.Title] = book
	}

	for _, book := range new {
		if _, ok := bookMap[book.Title]; !ok {
			bookMap[book.Title] = book
		}
	}

	result := make([]BookReferenceExtraction, 0, len(bookMap))
	for _, book := range bookMap {
		result = append(result, book)
	}

	return result
}

// needsLLMMerge determines if we need LLM for complex merge
func (c *ChunkedSyllabusExtractor) needsLLMMerge(original, merged []SubjectExtractionResult) bool {
	// If significant reduction in subjects, might have missed something
	if len(merged) < len(original)/2 && len(original) > 4 {
		return true
	}

	// Check for subjects with same code but different names (potential merge issues)
	codeNames := make(map[string][]string)
	for _, s := range original {
		codeNames[s.SubjectCode] = append(codeNames[s.SubjectCode], s.SubjectName)
	}

	for _, names := range codeNames {
		if len(names) > 1 {
			// Multiple different names for same code - needs LLM judgment
			uniqueNames := make(map[string]bool)
			for _, n := range names {
				uniqueNames[n] = true
			}
			if len(uniqueNames) > 1 {
				return true
			}
		}
	}

	return false
}

// llmMerge uses LLM to intelligently merge and deduplicate subjects
func (c *ChunkedSyllabusExtractor) llmMerge(
	ctx context.Context,
	partialSubjects []SubjectExtractionResult,
) (*SyllabusExtractionResult, error) {
	// Create timeout context for merge
	mergeCtx, cancel := context.WithTimeout(ctx, c.mergeTimeout)
	defer cancel()

	// Serialize partial subjects to JSON
	partialJSON, err := json.MarshalIndent(partialSubjects, "", "  ")
	if err != nil {
		return nil, fmt.Errorf("failed to serialize partial subjects: %w", err)
	}

	systemPrompt := `You are merging multiple partial extraction results from different pages of the same syllabus PDF into a single consolidated result.

Tasks:
1. Identify unique subjects by subject_code (e.g., MCA 301, MCA 302)
2. Merge units from the same subject that appeared in different chunks
3. Remove duplicate units (same unit_number for same subject)
4. Keep the most complete version of each unit (prefer longer raw_text)
5. Consolidate books lists (remove duplicates by title)
6. Ensure each subject has complete, ordered units (unit 1, 2, 3...)

Output ONLY valid JSON with the merged result:
{
  "subjects": [
    {
      "subject_name": "...",
      "subject_code": "...",
      "total_credits": N,
      "units": [...all units in order...],
      "books": [...deduplicated books...]
    }
  ]
}

Rules:
- Same subject_code = same subject (merge their units)
- Keep units ordered by unit_number
- Prefer longer/more complete raw_text when merging duplicate units
- Output ONLY JSON, no explanation`

	userPrompt := fmt.Sprintf("Merge these partial syllabus extractions into a single consolidated result:\n\n%s", string(partialJSON))

	response, err := c.inferenceClient.SimpleCompletion(
		mergeCtx,
		systemPrompt,
		userPrompt,
		digitalocean.WithInferenceMaxTokens(8192),
		digitalocean.WithInferenceTemperature(0.1),
	)
	if err != nil {
		log.Printf("ChunkedExtractor: LLM merge failed, falling back to programmatic merge: %v", err)
		// Fall back to programmatic merge
		merged := c.programmaticMerge(partialSubjects)
		return &SyllabusExtractionResult{Subjects: merged}, nil
	}

	var result SyllabusExtractionResult
	if err := utils.ExtractJSONTo(response, &result); err != nil {
		log.Printf("ChunkedExtractor: Failed to parse LLM merge response, falling back: %v", err)
		merged := c.programmaticMerge(partialSubjects)
		return &SyllabusExtractionResult{Subjects: merged}, nil
	}

	return &result, nil
}

// saveMultiSubjectSyllabusData saves multiple subjects to the database
func (c *ChunkedSyllabusExtractor) saveMultiSubjectSyllabusData(
	ctx context.Context,
	document *model.Document,
	extractedData *SyllabusExtractionResult,
) ([]*model.Syllabus, error) {
	var syllabuses []*model.Syllabus

	tx := c.db.Begin()
	if tx.Error != nil {
		return nil, tx.Error
	}

	defer func() {
		if r := recover(); r != nil {
			tx.Rollback()
		}
	}()

	for _, subjectData := range extractedData.Subjects {
		// Find or create subject by code
		var subject model.Subject
		subjectQuery := tx.Where("code = ?", subjectData.SubjectCode)

		if err := subjectQuery.First(&subject).Error; err != nil {
			if err == gorm.ErrRecordNotFound {
				// Create new subject
				subject = model.Subject{
					Name:       subjectData.SubjectName,
					Code:       subjectData.SubjectCode,
					SemesterID: document.Subject.SemesterID,
				}
				if err := tx.Create(&subject).Error; err != nil {
					tx.Rollback()
					return nil, fmt.Errorf("failed to create subject %s: %w", subjectData.SubjectCode, err)
				}
				log.Printf("ChunkedExtractor: Created new subject: %s (%s)", subject.Name, subject.Code)
			} else {
				tx.Rollback()
				return nil, fmt.Errorf("failed to query subject: %w", err)
			}
		}

		// Create syllabus for this subject
		syllabus := &model.Syllabus{
			SubjectID:        subject.ID,
			DocumentID:       document.ID,
			SubjectName:      subjectData.SubjectName,
			SubjectCode:      subjectData.SubjectCode,
			TotalCredits:     subjectData.TotalCredits,
			ExtractionStatus: model.SyllabusExtractionCompleted,
		}

		if err := tx.Create(syllabus).Error; err != nil {
			tx.Rollback()
			return nil, fmt.Errorf("failed to create syllabus: %w", err)
		}

		// Create units
		for _, unitData := range subjectData.Units {
			unit := model.SyllabusUnit{
				SyllabusID:  syllabus.ID,
				UnitNumber:  unitData.UnitNumber,
				Title:       unitData.Title,
				Description: unitData.Description,
				RawText:     unitData.RawText,
				Hours:       unitData.Hours,
			}

			if err := tx.Create(&unit).Error; err != nil {
				tx.Rollback()
				return nil, fmt.Errorf("failed to create unit: %w", err)
			}

			// Create topics
			for _, topicData := range unitData.Topics {
				topic := model.SyllabusTopic{
					UnitID:      unit.ID,
					TopicNumber: topicData.TopicNumber,
					Title:       topicData.Title,
					Description: topicData.Description,
					Keywords:    topicData.Keywords,
				}

				if err := tx.Create(&topic).Error; err != nil {
					tx.Rollback()
					return nil, fmt.Errorf("failed to create topic: %w", err)
				}
			}
		}

		// Create book references
		for _, bookData := range subjectData.Books {
			book := model.BookReference{
				SyllabusID: syllabus.ID,
				Title:      bookData.Title,
				Authors:    bookData.Authors,
				Publisher:  bookData.Publisher,
				Edition:    bookData.Edition,
				Year:       bookData.Year,
				ISBN:       bookData.ISBN,
				IsTextbook: bookData.IsTextbook,
				BookType:   bookData.BookType,
			}

			if err := tx.Create(&book).Error; err != nil {
				tx.Rollback()
				return nil, fmt.Errorf("failed to create book reference: %w", err)
			}
		}

		syllabuses = append(syllabuses, syllabus)
		log.Printf("ChunkedExtractor: Saved syllabus for %s (%s) with %d units",
			subjectData.SubjectName, subjectData.SubjectCode, len(subjectData.Units))
	}

	if err := tx.Commit().Error; err != nil {
		return nil, fmt.Errorf("failed to commit transaction: %w", err)
	}

	return syllabuses, nil
}

// extractTopicsFromRawText intelligently extracts individual topics from raw_text
// Splits on common separators: comma, dash (–), semicolon, newline
func extractTopicsFromRawText(rawText string) []Topic {
	if rawText == "" {
		return []Topic{}
	}

	// Replace common separators with a delimiter
	text := rawText
	text = strings.ReplaceAll(text, " – ", "|") // em-dash
	text = strings.ReplaceAll(text, " - ", "|") // regular dash
	text = strings.ReplaceAll(text, "; ", "|")   // semicolon
	text = strings.ReplaceAll(text, ", ", "|")   // comma
	text = strings.ReplaceAll(text, "\n", "|")   // newline

	// Split by delimiter
	parts := strings.Split(text, "|")

	// Clean and filter topics
	var topics []Topic
	seenTopics := make(map[string]bool)
	
	for i, part := range parts {
		// Trim whitespace
		topic := strings.TrimSpace(part)
		
		// Skip empty, very short, or already seen topics
		if topic == "" || len(topic) < 3 {
			continue
		}
		
		// Normalize for deduplication (lowercase)
		normalized := strings.ToLower(topic)
		if seenTopics[normalized] {
			continue
		}
		
		seenTopics[normalized] = true
		topics = append(topics, Topic{
			TopicNumber: i + 1,
			Title:       topic,
		})
	}

	// If no topics found or only one topic, return the raw text as a single topic
	if len(topics) <= 1 {
		return []Topic{{TopicNumber: 1, Title: rawText}}
	}

	return topics
}
